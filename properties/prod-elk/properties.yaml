# helm2.9.0 install . --name prometheus-elk --kube-context=prod-elk --namespace=monitor --tiller-namespace=monitor --values=properties/prod-elk/properties.yaml
# helm2.9.0 upgrade prometheus-elk . --kube-context=prod-elk --namespace=monitor --tiller-namespace=monitor --values=properties/prod-elk/properties.yaml
# helm2.9.0 delete --purge prometheus-elk --kube-context=prod-elk --tiller-namespace=monitor
rbac:
  create: true

pushgateway:
  enabled: false



################################################################################
# PROMETHEUS SERVER VALUES
################################################################################

server:
  # Prometheus data retention period (i.e 360h)
  retention: "96h"
  persistentVolume:
    enabled: true
    size: 8Gi
  resources:
    requests:
      cpu: 100m
      memory: 100Mi
    limits:
      cpu: 500m
      memory: 2500Mi
  extraArgs:
    # "log.level": debug


serverFiles:
  prometheus.yml:
    global:
      scrape_interval: 30s
      scrape_timeout: 30s
      evaluation_interval: 30s

    # A scrape configuration for running Prometheus on a Kubernetes cluster.
    # This uses separate scrape configs for cluster components (i.e. API server, node)
    # and services to allow each to use different authentication configs.
    #
    # Kubernetes labels will be added as Prometheus labels on metrics via the
    # `labelmap` relabeling action.
    #
    # If you are using Kubernetes 1.7.2 or earlier, please take note of the comments
    # for the kubernetes-cadvisor job; you will need to edit or remove this job.

    # Scrape config for API servers.
    #
    # Kubernetes exposes API servers as endpoints to the default/kubernetes
    # service so this uses `endpoints` role and uses relabelling to only keep
    # the endpoints associated with the default/kubernetes service using the
    # default named port `https`. This works for single API server deployments as
    # well as HA API server deployments.
    scrape_configs:
    - job_name: 'kubernetes-apiservers'

      kubernetes_sd_configs:
      - role: endpoints

      # Default to scraping over https. If required, just disable this or change to
      # `http`.
      scheme: https

      # This TLS & bearer token file config is used to connect to the actual scrape
      # endpoints for cluster components. This is separate to discovery auth
      # configuration because discovery & scraping are two separate concerns in
      # Prometheus. The discovery auth config is automatic if Prometheus runs inside
      # the cluster. Otherwise, more config options have to be provided within the
      # <kubernetes_sd_config>.
      tls_config:
        ca_file: /var/run/secrets/kubernetes.io/serviceaccount/ca.crt
        # If your node certificates are self-signed or use a different CA to the
        # master CA, then disable certificate verification below. Note that
        # certificate verification is an integral part of a secure infrastructure
        # so this should only be disabled in a controlled environment. You can
        # disable certificate verification by uncommenting the line below.
        #
        # insecure_skip_verify: true
      bearer_token_file: /var/run/secrets/kubernetes.io/serviceaccount/token

      # Keep only the default/kubernetes service endpoints for the https port. This
      # will add targets for each API server which Kubernetes adds an endpoint to
      # the default/kubernetes service.
      relabel_configs:
      - source_labels: [__meta_kubernetes_namespace, __meta_kubernetes_service_name, __meta_kubernetes_endpoint_port_name]
        action: keep
        regex: default;kubernetes;https

    # Scrape config for nodes (kubelet).
    #
    # Rather than connecting directly to the node, the scrape is proxied though the
    # Kubernetes apiserver.  This means it will work if Prometheus is running out of
    # cluster, or can't connect to nodes for some other reason (e.g. because of
    # firewalling).
    - job_name: 'kubernetes-nodes'

      # Default to scraping over https. If required, just disable this or change to
      # `http`.
      scheme: https

      # This TLS & bearer token file config is used to connect to the actual scrape
      # endpoints for cluster components. This is separate to discovery auth
      # configuration because discovery & scraping are two separate concerns in
      # Prometheus. The discovery auth config is automatic if Prometheus runs inside
      # the cluster. Otherwise, more config options have to be provided within the
      # <kubernetes_sd_config>.
      tls_config:
        ca_file: /var/run/secrets/kubernetes.io/serviceaccount/ca.crt
      bearer_token_file: /var/run/secrets/kubernetes.io/serviceaccount/token

      kubernetes_sd_configs:
      - role: node

      relabel_configs:
      - action: labelmap
        regex: __meta_kubernetes_node_label_(.+)
      - target_label: __address__
        replacement: kubernetes.default.svc:443
      - source_labels: [__meta_kubernetes_node_name]
        regex: (.+)
        target_label: __metrics_path__
        replacement: /api/v1/nodes/${1}/proxy/metrics

    # Scrape config for Kubelet cAdvisor.
    #
    # This is required for Kubernetes 1.7.3 and later, where cAdvisor metrics
    # (those whose names begin with 'container_') have been removed from the
    # Kubelet metrics endpoint.  This job scrapes the cAdvisor endpoint to
    # retrieve those metrics.
    #
    # In Kubernetes 1.7.0-1.7.2, these metrics are only exposed on the cAdvisor
    # HTTP endpoint; use "replacement: /api/v1/nodes/${1}:4194/proxy/metrics"
    # in that case (and ensure cAdvisor's HTTP server hasn't been disabled with
    # the --cadvisor-port=0 Kubelet flag).
    #
    # This job is not necessary and should be removed in Kubernetes 1.6 and
    # earlier versions, or it will cause the metrics to be scraped twice.
    - job_name: 'kubernetes-cadvisor'

      # Default to scraping over https. If required, just disable this or change to
      # `http`.
      scheme: https

      # This TLS & bearer token file config is used to connect to the actual scrape
      # endpoints for cluster components. This is separate to discovery auth
      # configuration because discovery & scraping are two separate concerns in
      # Prometheus. The discovery auth config is automatic if Prometheus runs inside
      # the cluster. Otherwise, more config options have to be provided within the
      # <kubernetes_sd_config>.
      tls_config:
        ca_file: /var/run/secrets/kubernetes.io/serviceaccount/ca.crt
      bearer_token_file: /var/run/secrets/kubernetes.io/serviceaccount/token

      kubernetes_sd_configs:
      - role: node

      relabel_configs:
      - action: labelmap
        regex: __meta_kubernetes_node_label_(.+)
      - target_label: __address__
        replacement: kubernetes.default.svc:443
      - source_labels: [__meta_kubernetes_node_name]
        regex: (.+)
        target_label: __metrics_path__
        replacement: /api/v1/nodes/${1}/proxy/metrics/cadvisor

    # Example scrape config for pods
    #
    # The relabeling allows the actual pod scrape endpoint to be configured via the
    # following annotations:
    #
    # * `prometheus.io/scrape`: Only scrape pods that have a value of `true`
    # * `prometheus.io/path`: If the metrics path is not `/metrics` override this.
    # * `prometheus.io/port`: Scrape the pod on the indicated port instead of the
    #   pod's declared ports (default is a port-free target if none are declared).
    - job_name: 'kubernetes-pods'

      kubernetes_sd_configs:
      - role: pod

      relabel_configs:
      - source_labels: [__meta_kubernetes_pod_annotation_prometheus_io_scrape]
        action: keep
        regex: true
      - source_labels: [__meta_kubernetes_pod_annotation_prometheus_io_path]
        action: replace
        target_label: __metrics_path__
        regex: (.+)
      - source_labels: [__address__, __meta_kubernetes_pod_annotation_prometheus_io_port]
        action: replace
        regex: ([^:]+)(?::\d+)?;(\d+)
        replacement: $1:$2
        target_label: __address__
      - action: labelmap
        regex: __meta_kubernetes_pod_label_(.+)
      - source_labels: [__meta_kubernetes_namespace]
        action: replace
        target_label: kubernetes_namespace
      - source_labels: [__meta_kubernetes_pod_name]
        action: replace
        target_label: kubernetes_pod_name

    # Scrape config for service endpoints.
    #
    # The relabeling allows the actual service scrape endpoint to be configured
    # via the following annotations:
    #
    # * `prometheus.io/scrape`: Only scrape services that have a value of `true`
    # * `prometheus.io/scheme`: If the metrics endpoint is secured then you will need
    # to set this to `https` & most likely set the `tls_config` of the scrape config.
    # * `prometheus.io/path`: If the metrics path is not `/metrics` override this.
    # * `prometheus.io/port`: If the metrics are exposed on a different port to the
    # service then set this appropriately.
    - job_name: 'kubernetes-service-endpoints'

      kubernetes_sd_configs:
      - role: endpoints

      relabel_configs:
      - source_labels: [__meta_kubernetes_service_annotation_prometheus_io_scrape]
        action: keep
        regex: true
      - source_labels: [__meta_kubernetes_service_annotation_prometheus_io_scheme]
        action: replace
        target_label: __scheme__
        regex: (https?)
      - source_labels: [__meta_kubernetes_service_annotation_prometheus_io_path]
        action: replace
        target_label: __metrics_path__
        regex: (.+)
      - source_labels: [__address__, __meta_kubernetes_service_annotation_prometheus_io_port]
        action: replace
        target_label: __address__
        regex: ([^:]+)(?::\d+)?;(\d+)
        replacement: $1:$2
      - action: labelmap
        regex: __meta_kubernetes_service_label_(.+)
      - source_labels: [__meta_kubernetes_namespace]
        action: replace
        target_label: kubernetes_namespace
      - source_labels: [__meta_kubernetes_service_name]
        action: replace
        target_label: kubernetes_name


  # https://github.com/coreos/prometheus-operator/blob/master/contrib/kube-prometheus/manifests/prometheus/prometheus-k8s-rules.yaml
  rules:
    groups:

    ############################################################################
    # NODE ALERT RULES
    ############################################################################
    - name: node.rules
      rules:
      - alert: NodeDiskRunningFullIn24Hours
        expr: predict_linear(node_filesystem_free[6h], 3600 * 24) < 0
        for: 30m
        labels:
          severity: warning
        annotations:
          description: device {{$labels.device}} on node {{$labels.instance}} is running
            full within the next 24 hours (mounted at {{$labels.mountpoint}})
      - alert: NodeDiskRunningFullIn2Hours
        expr: predict_linear(node_filesystem_free[30m], 3600 * 2) < 0
        for: 10m
        labels:
          severity: alert
        annotations:
          description: device {{$labels.device}} on node {{$labels.instance}} is running
            full within the next 2 hours (mounted at {{$labels.mountpoint}})
          summary: Node disk is running full
      - alert: NodeMemoryUsage
        expr: (((node_memory_MemTotal-node_memory_MemAvailable)/(node_memory_MemTotal)*100)) > 75
        for: 2m
        labels:
          severity: alert
        annotations:
          description: "{{$labels.instance}}: High memory usage detected"
          summary: "{{$labels.instance}}: Memory usage is above 75% (current value is: {{ $value }})"
      - alert: NodeLoadAverage
        expr: (node_load5 / count without (cpu, mode) (node_cpu{mode="system"})) > 1
        for: 2m
        labels:
          severity: warning
        annotations:
          description: "{{$labels.instance}}: High load average detected"
          summary: "{{$labels.instance}}: Load average is high"


    ############################################################################
    # POD ALERT RULES
    ############################################################################
    - name: pod.rules
      rules:

      # Alert for any pod that is down for >5 minutes.
      - alert: PodDown
        expr: up{job="kubernetes-pods"} == 0
        for: 1m
        labels:
          severity: alert
        annotations:
          summary: "Pod {{ $labels.kubernetes_pod_name }} down."
          description: "Pod {{ $labels.kubernetes_pod_name }} in namespace {{ $labels.kubernetes_namespace }} down for more than 5 minutes."
      # Alert for any instance that has a median request latency >1s.
      - alert: APIHighRequestLatency
        expr: api_http_request_latencies_second{quantile="0.5"} > 1
        for: 10m
        labels:
          severity: warning
        annotations:
          summary: "High request latency on {{ $labels.instance }}"
          description: "{{ $labels.instance }} has a median request latency above 1s (current value: {{ $value }}s)"



    ############################################################################
    # ELASTICSEARCH ALERT RULES
    ############################################################################
    - name: elasticsearch.rules
      rules:

      - alert: ElasticSearchClusterHealthYellow
        expr: elasticsearch_cluster_health_status{color="yellow"} == 1
        for: 5m
        labels:
          severity: warning
        annotations:
          summary: "ElasticSearch cluster health has been \"yellow\" for more than 5 minutes."
          description: "ElasticSearch cluster health has been \"yellow\" for more than 5 minutes. Needs repair."

      - alert: ElasticSearchClusterHealthRed
        expr: elasticsearch_cluster_health_status{color="red"} == 1
        for: 5m
        labels:
          severity: emergency
        annotations:
          summary: "ElasticSearch cluster health has been \"red\" for more than 5 minutes."
          description: "ElasticSearch cluster health has been \"red\" for more than 5 minutes. Needs repair."

      - alert: ElasticSearchInusfficientNumDataNodes
        expr: elasticsearch_cluster_health_number_of_data_nodes < 2
        for: 5m
        labels:
          severity: emergency
        annotations:
          summary: "Insufficient number of running ElasticSearch data nodes"
          description: "Only {{ $value }} ElasticSearch data nodes are running. >=2 required."

      - alert: ElasticSearchHeapUsage
        expr: elasticsearch_jvm_memory_used_bytes{area="heap"} / elasticsearch_jvm_memory_max_bytes{area="heap"} > 0.9
        for: 15m
        labels:
          severity: warning
        annotations:
          summary: "ElasticSearch heap usage over 90% for 15m"
          description: "ElasticSearch node {{$labels.name}}: Heap usage 90% for 15m (current value: {{ $value }}%)"

      - alert: ElasticSearchDiskRunningFullIn24Hours
        expr: predict_linear(elasticsearch_filesystem_data_free_bytes[6h], 3600 * 24) < 0
        for: 30m
        labels:
          severity: warning
        annotations:
          summary: "ElasticSearch node {{$labels.name}} running full within the next 24 hours"
          description: "ElasticSearch node {{$labels.name}}: Data volume usage is above is running full within the next 24 hours (mounted at {{$labels.mount}})"

      - alert: ElasticSearchDiskRunningFullIn2Hours
        expr: predict_linear(elasticsearch_filesystem_data_free_bytes[30m], 3600 * 2) < 0
        for: 10m
        labels:
          severity: alert
        annotations:
          summary: "ElasticSearch node {{$labels.name}} running full within the next 2 hours"
          description: "ElasticSearch node {{$labels.name}}: Data volume usage is above is running full within the next 2 hours (mounted at {{$labels.mount}})"

      - alert: ElasticSearchDiskRunningFull80Percent
        expr: 100 - (elasticsearch_filesystem_data_available_bytes / elasticsearch_filesystem_data_size_bytes) * 100 > 80
        for: 2m
        labels:
          severity: warning
        annotations:
          summary: "ElasticSearch node {{$labels.name}}: Data volume usage >80% detected"
          description: "ElasticSearch node {{$labels.name}}: Data volume usage is {{ $value }}% (mounted at {{$labels.mount}})"

      - alert: ElasticSearchDiskRunningFull90Percent
        expr: 100 - (elasticsearch_filesystem_data_available_bytes / elasticsearch_filesystem_data_size_bytes) * 100 > 90
        for: 2m
        labels:
          severity: alert
        annotations:
          summary: "ElasticSearch node {{$labels.name}}: Data volume usage >90% detected"
          description: "ElasticSearch node {{$labels.name}}: Data volume usage is {{ $value }}% (mounted at {{$labels.mount}})"

    ############################################################################
    # GENERAL ALERT RULES
    ############################################################################
    - name: general.rules
      rules:
      - alert: TargetDown
        # Skip kubernetes-apiservers for now (NOTE: maybe revisit later)
        # If more than 10% of targets for Prometheus jobs are down then alert
        expr: 100 * (count(up{job!="kubernetes-apiservers"} == 0) BY (job) / count(up{job!="kubernetes-apiservers"}) BY (job)) > 10
        for: 10m
        labels:
          severity: warning
        annotations:
          description: "{{ $value }}% of Prometheus job \"{{ $labels.job }}\" targets are down."
          summary: "Targets are down"


    ############################################################################
    # KUBELET (cadvisor) ALERT RULES
    ############################################################################
    - name: kubelet.rules
      rules:
      - alert: K8SNodeNotReady
        expr: kube_node_status_condition{condition="Ready",status="true"} == 0
        for: 1h
        labels:
          severity: warning
        annotations:
          description: The Kubelet on {{ $labels.node }} has not checked in with the API,
            or has set itself to NotReady, for more than an hour
          summary: Node status is NotReady
      - alert: K8SManyNodesNotReady
        expr: count(kube_node_status_condition{condition="Ready",status="true"} == 0)
          > 1 and (count(kube_node_status_condition{condition="Ready",status="true"} ==
          0) / count(kube_node_status_condition{condition="Ready",status="true"})) > 0.2
        for: 1m
        labels:
          severity: alert
        annotations:
          description: "{{ $value }}% of Kubernetes nodes are not ready"
      - alert: K8SKubeletDown
        expr: count(up{job="kubelet"} == 0) / count(up{job="kubelet"}) * 100 > 3
        for: 1h
        labels:
          severity: warning
        annotations:
          description: "Prometheus failed to scrape {{ $value }}% of kubelets."
          summary: Prometheus failed to scrape
      - alert: K8SKubeletTooManyPods
        expr: kubelet_running_pod_count > 100
        for: 10m
        labels:
          severity: warning
        annotations:
          description: Kubelet {{$labels.instance}} is running {{$value}} pods, close
            to the limit of 110
          summary: Kubelet is close to pod limit
      # - alert: K8SKubeletMissing
      #   expr: (absent(up{job="kubelet"} == 1) or count(up{job="kubelet"} == 0) / count(up{job="kubelet"}))
      #     * 100 > 10
      #   for: 1h
      #   labels:
      #     severity: error
      #   annotations:
      #     description: Prometheus failed to scrape {{ $value }}% of kubelets, or all Kubelets
      #       have disappeared from service discovery.
      #     summary: Many Kubelets cannot be scraped


    ############################################################################
    # KUBE STATE METRICS ALERT RULES
    ############################################################################
    - name: kube-state-metrics.rules
      rules:
      - alert: DeploymentGenerationMismatch
        expr: kube_deployment_status_observed_generation != kube_deployment_metadata_generation
        for: 15m
        labels:
          severity: warning
        annotations:
          description: Observed deployment generation does not match expected one for
            deployment {{$labels.namespace}}/{{$labels.deployment}}
          summary: Deployment is outdated
      - alert: DeploymentReplicasNotUpdated
        expr: ((kube_deployment_status_replicas_updated != kube_deployment_spec_replicas)
          or (kube_deployment_status_replicas_available != kube_deployment_spec_replicas))
          unless (kube_deployment_spec_paused == 1)
        for: 15m
        labels:
          severity: warning
        annotations:
          description: Replicas are not updated and available for deployment {{$labels.namespace}}/{{$labels.deployment}}
          summary: Deployment replicas are outdated
      - alert: DaemonSetRolloutStuck
        expr: kube_daemonset_status_number_ready / kube_daemonset_status_desired_number_scheduled
          * 100 < 100
        for: 15m
        labels:
          severity: warning
        annotations:
          description: Only {{$value}}% of desired pods scheduled and ready for daemon
            set {{$labels.namespace}}/{{$labels.daemonset}}
          summary: DaemonSet is missing pods
      - alert: K8SDaemonSetsNotScheduled
        expr: kube_daemonset_status_desired_number_scheduled - kube_daemonset_status_current_number_scheduled
          > 0
        for: 10m
        labels:
          severity: warning
        annotations:
          description: A number of daemonsets are not scheduled.
          summary: Daemonsets are not scheduled correctly
      - alert: DaemonSetsMisscheduled
        expr: kube_daemonset_status_number_misscheduled > 0
        for: 10m
        labels:
          severity: warning
        annotations:
          description: A number of daemonsets are running where they are not supposed
            to run.
          summary: Daemonsets are not scheduled correctly
      - alert: PodFrequentlyRestarting
        expr: increase(kube_pod_container_status_restarts_total[1h]) > 5
        for: 10m
        labels:
          severity: warning
        annotations:
          description: Pod {{$labels.namespace}}/{{$labels.pod}} was restarted {{$value}}
            times within the last hour
          summary: Pod is restarting frequently


    ############################################################################
    # PROMETHEUS ALERT RULES
    ############################################################################
    - name: prometheus.rules
      rules:
      - alert: PrometheusConfigReloadFailed
        expr: prometheus_config_last_reload_successful == 0
        for: 10m
        labels:
          severity: warning
        annotations:
          description: Reloading Prometheus' configuration has failed for {{$labels.namespace}}/{{$labels.pod}}
          summary: Reloading Promehteus' configuration failed

      - alert: PrometheusNotificationQueueRunningFull
        expr: predict_linear(prometheus_notifications_queue_length[5m], 60 * 30) > prometheus_notifications_queue_capacity
        for: 10m
        labels:
          severity: warning
        annotations:
          description: Prometheus' alert notification queue is running full for {{$labels.namespace}}/{{
            $labels.pod}}
          summary: Prometheus' alert notification queue is running full

      - alert: PrometheusErrorSendingAlerts
        expr: rate(prometheus_notifications_errors_total[5m]) / rate(prometheus_notifications_sent_total[5m])
          > 0.01
        for: 10m
        labels:
          severity: error
        annotations:
          description: Errors while sending alerts from Prometheus {{$labels.namespace}}/{{
            $labels.pod}} to Alertmanager {{$labels.Alertmanager}}
          summary: Errors while sending alert from Prometheus

      - alert: PrometheusErrorSendingAlerts
        expr: rate(prometheus_notifications_errors_total[5m]) / rate(prometheus_notifications_sent_total[5m])
          > 0.03
        for: 10m
        labels:
          severity: error
        annotations:
          description: Errors while sending alerts from Prometheus {{$labels.namespace}}/{{
            $labels.pod}} to Alertmanager {{$labels.Alertmanager}}
          summary: Errors while sending alerts from Prometheus

      - alert: PrometheusNotConnectedToAlertmanagers
        expr: prometheus_notifications_alertmanagers_discovered < 1
        for: 10m
        labels:
          severity: error
        annotations:
          description: Prometheus {{ $labels.namespace }}/{{ $labels.pod}} is not connected
            to any Alertmanagers
          summary: Prometheus is not connected to any Alertmanagers

      - alert: PrometheusTSDBReloadsFailing
        expr: increase(prometheus_tsdb_reloads_failures_total[2h]) > 0
        for: 12h
        labels:
          severity: warning
        annotations:
          description: '{{$labels.job}} at {{$labels.instance}} had {{$value | humanize}}
            reload failures over the last four hours.'
          summary: Prometheus has issues reloading data blocks from disk

      - alert: PrometheusTSDBCompactionsFailing
        expr: increase(prometheus_tsdb_compactions_failed_total[2h]) > 0
        for: 12h
        labels:
          severity: warning
        annotations:
          description: '{{$labels.job}} at {{$labels.instance}} had {{$value | humanize}}
            compaction failures over the last four hours.'
          summary: Prometheus has issues compacting sample blocks

      - alert: PrometheusTSDBWALCorruptions
        expr: tsdb_wal_corruptions_total > 0
        for: 4h
        labels:
          severity: warning
        annotations:
          description: '{{$labels.job}} at {{$labels.instance}} has a corrupted write-ahead
            log (WAL).'
          summary: Prometheus write-ahead log is corrupted

      - alert: PrometheusNotIngestingSamples
        expr: rate(prometheus_tsdb_head_samples_appended_total[5m]) <= 0
        for: 10m
        labels:
          severity: warning
        annotations:
          description: "Prometheus {{ $labels.namespace }}/{{ $labels.pod}} isn't ingesting samples."
          summary: "Prometheus isn't ingesting samples"


    ############################################################################
    # ALERTMANAGER ALERT RULES
    ############################################################################
    - name: alertmanager.rules
      rules:
      - alert: AlertmanagerConfigInconsistent
        expr: count_values("config_hash", alertmanager_config_hash) BY (service) / ON(service)
          GROUP_LEFT() label_replace(prometheus_operator_alertmanager_spec_replicas, "service",
          "alertmanager-$1", "alertmanager", "(.*)") != 1
        for: 5m
        labels:
          severity: error
        annotations:
          description: The configuration of the instances of the Alertmanager cluster
            `{{$labels.service}}` are out of sync.
          summary: Configuration out of sync
      - alert: AlertmanagerDownOrMissing
        expr: label_replace(prometheus_operator_alertmanager_spec_replicas, "job", "alertmanager-$1",
          "alertmanager", "(.*)") / ON(job) GROUP_RIGHT() sum(up) BY (job) != 1
        for: 5m
        labels:
          severity: warning
        annotations:
          description: An unexpected number of Alertmanagers are scraped or Alertmanagers
            disappeared from discovery.
          summary: Alertmanager down or missing
      - alert: AlertmanagerFailedReload
        expr: alertmanager_config_last_reload_successful == 0
        for: 10m
        labels:
          severity: warning
        annotations:
          description: Reloading Alertmanager's configuration has failed for {{ $labels.namespace
            }}/{{ $labels.pod}}.
          summary: Alertmanager's configuration reload failed


    ############################################################################
    # K8S API ALERT RULES
    ############################################################################
    - name: kubernetes-api-server.rules
      rules:
      - alert: APIServerLatencyHigh
        expr: apiserver_latency_seconds:quantile{quantile="0.99",subresource!="log",verb!~"^(?:WATCH|WATCHLIST|PROXY|CONNECT)$"}
          > 1
        for: 10m
        labels:
          severity: warning
        annotations:
          description: the API server has a 99th percentile latency of {{ $value }} seconds
            for {{$labels.verb}} {{$labels.resource}}
          summary: API server high latency
      - alert: APIServerLatencyHigh
        expr: apiserver_latency_seconds:quantile{quantile="0.99",subresource!="log",verb!~"^(?:WATCH|WATCHLIST|PROXY|CONNECT)$"}
          > 4
        for: 10m
        labels:
          severity: critical
        annotations:
          description: the API server has a 99th percentile latency of {{ $value }} seconds
            for {{$labels.verb}} {{$labels.resource}}
          summary: API server high latency
      - alert: APIServerErrorsHigh
        expr: rate(apiserver_request_count{code=~"^(?:5..)$"}[5m]) / rate(apiserver_request_count[5m])
          * 100 > 2
        for: 10m
        labels:
          severity: warning
        annotations:
          description: API server returns errors for {{ $value }}% of requests
          summary: API server request errors
      - alert: APIServerErrorsHigh
        expr: rate(apiserver_request_count{code=~"^(?:5..)$"}[5m]) / rate(apiserver_request_count[5m])
          * 100 > 5
        for: 10m
        labels:
          severity: critical
        annotations:
          description: API server returns errors for {{ $value }}% of requests
      - alert: K8SApiserverDown
        expr: absent(up{job="apiserver"} == 1)
        for: 1d
        labels:
          severity: warning
        annotations:
          description: No API servers are reachable or all have disappeared from service
            discovery
          summary: No API servers are reachable


    ############################################################################
    # TEST ALERT RULES
    ############################################################################
    - name: test.rules
      #interval: 10s
      rules:
      - alert: TestAlert
        expr: up{job="kubernetes-pods"} == 1
        labels:
          severity: notice
          alerttype: test
        annotations:
          description: Testing TestAlert.
          summary: "Pod \"{{ $labels.kubernetes_pod_name }}\" in namespace \"{{ $labels.kubernetes_namespace }}\" is up."
      # - alert: TestCustomAlert
      #   expr: up == 1
      #   labels:
      #     severity: notice
      #     alerttype: test
      #     slack_recipient: '@gian.costa'
      #   annotations:
      #     description: Testing TestAlert.
      #     summary: This is a TestAlert meant to ensure that the entire Alerting pipeline is functional.
      # - alert: TestCustomAlert2
      #   expr: up == 1
      #   labels:
      #     severity: emergency
      #     alerttype: test
      #   annotations:
      #     description: Testing colored alerts.
      #     summary: Color colors colors.



################################################################################
# ALERTMANAGER VALUES
################################################################################

alertmanager:
  enabled: true
  persistentVolume:
    enabled: true
    #existingClaim: "alertmanager-pvc"
  extraArgs:
    #"log.level": debug


alertmanagerFiles:

  alertmanager.yml:

    global:
      resolve_timeout: 30s

    route:
      receiver: 'slack-notifications-prod-elk-alerts-yellow'
      group_by: [alertname]
      group_wait: 30s
      group_interval: 5m
      repeat_interval: 3h

      # The child route trees.
      routes:

      - match_re:
          alerttype: ^test.*$
        receiver: slack-notifications-test
        # Send subsequent test alerts more often than every 5 minutes
        group_interval: 10s
        routes:
        - match_re:
            alerttype: ^test.*$
          receiver: slack-notifications-test
          continue: true
        # - match_re:
        #     alerttype: ^test.*$
        #   receiver: 'email-gian'
        #   continue: true

      - match:
          alerttype: custom
        receiver: slack-notifications-custom

      - match_re:
          severity: ^(debug.*|info.*|notice.*)$
        receiver: slack-notifications-prod-elk-alerts-blue

      - match_re:
          severity: ^(warn.*)$
        receiver: slack-notifications-prod-elk-alerts-yellow

      - match_re:
          severity: ^(err.*|crit.*|alert.*|emerg.*)$
        receiver: slack-notifications-prod-elk-alerts-red
        routes:
        - match_re:
            severity: ^.*$
          receiver: slack-notifications-prod-elk-alerts-red
          continue: true
        # TODO add
        # - match_re:
        #     severity: ^(alert.*|emerg.*)$
        #   receiver: email-IT
        #   continue: true

    receivers:

    - name: 'slack-notifications-prod-elk-alerts-blue'
      slack_configs:
      - channel: '{{if .CommonLabels.slack_recipient }}{{.CommonLabels.slack_recipient}}{{else}}#prod_elk_alerts{{end}}'
        api_url: 'SLACK_HOOK_URL_HERE'
        icon_emoji: ':loudspeaker:'
        color: '#33DDFF'
        username: 'Notification (Prometheus)'
        pretext: 'Prod ELK'
        title: '{{ .GroupLabels.alertname }} Alerts'
        text: |-
          {{ range .Alerts }}

          > `Alert`
          > *Severity*: _{{ .Labels.severity }}_
          > *Summary*: {{ .Annotations.summary }}
          > *Description*: {{ .Annotations.description }}

          {{ end }}

    - name: 'slack-notifications-prod-elk-alerts-yellow'
      slack_configs:
      - channel: '{{if .CommonLabels.slack_recipient }}{{.CommonLabels.slack_recipient}}{{else}}#prod_elk_alerts{{end}}'
        api_url: 'SLACK_HOOK_URL_HERE'
        icon_emoji: ':warning:'
        color: 'warning'
        username: 'Yellow Alert (Prometheus)'
        pretext: 'Prod ELK'
        title: '{{ .GroupLabels.alertname }} Alerts'
        text: |-
          {{ range .Alerts }}

          > `Alert`
          > *Severity*: _{{ .Labels.severity }}_
          > *Summary*: {{ .Annotations.summary }}
          > *Description*: {{ .Annotations.description }}

          {{ end }}

    - name: 'slack-notifications-prod-elk-alerts-red'
      slack_configs:
      - channel: '{{if .CommonLabels.slack_recipient }}{{.CommonLabels.slack_recipient}}{{else}}#prod_elk_alerts{{end}}'
        api_url: 'SLACK_HOOK_URL_HERE'
        icon_emoji: ':rotating_light:'
        color: 'danger'
        username: 'Red Alert (Prometheus)'
        pretext: 'Prod ELK'
        title: '{{ .GroupLabels.alertname }} Alerts'
        text: |-
          {{ range .Alerts }}

          > `Alert`
          > *Severity*: _{{ .Labels.severity }}_
          > *Summary*: {{ .Annotations.summary }}
          > *Description*: {{ .Annotations.description }}

          {{ end }}

    - name: 'slack-notifications-custom'
      slack_configs:
      - channel: '{{if .CommonLabels.slack_recipient }}{{.CommonLabels.slack_recipient}}{{else}}#prod_elk_alerts{{end}}'
        api_url: 'SLACK_HOOK_URL_HERE'
        icon_emoji: '{{if or (eq .CommonLabels.severity "emergency") (eq .CommonLabels.severity "alert") (eq .CommonLabels.severity "critical") (eq .CommonLabels.severity "error")}}:rotating_light:{{else}}{{if eq .CommonLabels.severity "warning"}}:warning:{{else}}:loudspeaker:{{end}}{{end}}'
        color: '{{if or (eq .CommonLabels.severity "emergency") (eq .CommonLabels.severity "alert") (eq .CommonLabels.severity "critical") (eq .CommonLabels.severity "error")}}danger{{else}}{{if eq .CommonLabels.severity "warning"}}warning{{else}}#33DDFF{{end}}{{end}}'
        username: '{{if or (eq .CommonLabels.severity "emergency") (eq .CommonLabels.severity "alert") (eq .CommonLabels.severity "critical") (eq .CommonLabels.severity "error")}}Red Alert{{else}}{{if eq .CommonLabels.severity "warning"}}Yellow Alert{{else}}Notification{{end}}{{end}} (Custom) (Prometheus)'
        pretext: 'Prod ELK'
        title: '{{ .GroupLabels.alertname }} Alerts'
        text: |-
          {{ range .Alerts }}

          > `Alert`
          > *Severity*: _{{ .Labels.severity }}_
          > *Summary*: {{ .Annotations.summary }}
          > *Description*: {{ .Annotations.description }}

          {{ end }}

    - name: 'slack-notifications-test'
      slack_configs:
      - channel: '{{if .CommonLabels.slack_recipient }}{{.CommonLabels.slack_recipient}}{{else}}#test_jud{{end}}'
        api_url: 'SLACK_HOOK_URL_HERE'
        icon_emoji: '{{if or (eq .CommonLabels.severity "emergency") (eq .CommonLabels.severity "alert") (eq .CommonLabels.severity "critical") (eq .CommonLabels.severity "error")}}:rotating_light:{{else}}{{if eq .CommonLabels.severity "warning"}}:warning:{{else}}:loudspeaker:{{end}}{{end}}'
        color: '{{if or (eq .CommonLabels.severity "emergency") (eq .CommonLabels.severity "alert") (eq .CommonLabels.severity "critical") (eq .CommonLabels.severity "error")}}danger{{else}}{{if eq .CommonLabels.severity "warning"}}warning{{else}}#33DDFF{{end}}{{end}}'
        username: '{{if or (eq .CommonLabels.severity "emergency") (eq .CommonLabels.severity "alert") (eq .CommonLabels.severity "critical") (eq .CommonLabels.severity "error")}}Red Alert{{else}}{{if eq .CommonLabels.severity "warning"}}Yellow Alert{{else}}Notification{{end}}{{end}} (Test) (Prometheus)'
        pretext: 'Prod ELK Prometheus Alertmanager test'
        title: '{{ .GroupLabels.alertname }} Alerts'
        text: |-
          {{ range .Alerts }}

          > `Alert`
          > *Severity*: _{{ .Labels.severity }}_
          > *Summary*: {{ .Annotations.summary }}
          > *Description*: {{ .Annotations.description }}

          {{ end }}
    #
    # #
    # - name: 'email-IT'
    #   email_configs:
    #   - smarthost: 'smtp.dc2.ec.loc:25'
    #     to: 'it911@everyonecounts.com'
    #     from: 'prometheus.alertmanager@everyonecounts.com'
    #     require_tls: false
    #
    # #
    # - name: 'email-gian'
    #   email_configs:
    #   - smarthost: 'smtp.dc2.ec.loc:25'
    #     to: 'gian.costa@everyonecounts.com'
    #     from: 'prometheus.alertmanager@everyonecounts.com'
    #     require_tls: false



################################################################################
# PROMETHEUS NODE_EXPORTER VALUES
################################################################################

# Grafana Dashboard: https://grafana.com/dashboards/1860
nodeExporter:
  enabled: true
  image:
    tag: v0.16.0
  # Make sure prometheus scrapes this pods metrics!
  podAnnotations:
    prometheus.io/scrape: "true"
  securityContext:
    privileged: true
  extraArgs:
  #   "log.level": debug



################################################################################
# PROMETHEUS KUBE-STATE-METRICS VALUES
################################################################################

# Grafana Dashboard: https://grafana.com/dashboards/741
kubeStateMetrics:
  # Make sure prometheus scrapes this pods metrics!
  podAnnotations:
    prometheus.io/scrape: "true"
  image:
    tag: v1.2.0
